"""
RAG Retrieval System - Combines vector search with LLM generation

Usage:
    from src.rag.vector_store import LegalVectorStore
    from src.rag.retrieval import LegalRAGRetriever
    
    vector_store = LegalVectorStore(db_path="data/vector_db/chroma_db")
    rag = LegalRAGRetriever(vector_store)
    result = rag.query("Ù…Ø§ Ù‡ÙŠ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø³Ø±Ù‚Ø©ØŸ")
    print(result['answer'])
"""

import os
from typing import List, Dict, Optional
import logging
from openai import OpenAI

from .vector_store import LegalVectorStore

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LegalRAGRetriever:
    """
    Ù†Ø¸Ø§Ù… RAG ÙƒØ§Ù…Ù„ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
    ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ (Vector Search) ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª (LLM)
    """
    
    def __init__(self, vector_store: LegalVectorStore, model: str = "gpt-4o-mini", temperature: float = 0.3):
        """
        Args:
            vector_store: Vector database instance
            model: OpenAI model to use (gpt-4o-mini, gpt-4, gpt-3.5-turbo)
            temperature: Temperature for LLM (0-1, lower = more deterministic)
        """
        self.vector_store = vector_store
        self.model = model
        self.temperature = temperature
        
        # Initialize OpenAI client
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("âš ï¸  OPENAI_API_KEY not set. RAG queries will fail.")
            logger.warning("Set it with: export OPENAI_API_KEY='your-key-here'")
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        
        self.client = OpenAI(api_key=api_key)
        logger.info(f"âœ… RAG Retriever initialized with model: {model}")
    
    def retrieve(self, query: str, n_results: int = 5, filter_dict: Optional[Dict] = None) -> Dict:
        """
        Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Vector Database
        
        Args:
            query: Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            n_results: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            filter_dict: ÙÙ„ØªØ± Ø­Ø³Ø¨ metadata (Ù…Ø«Ù„Ø§Ù‹ {"chunk_type": "article"})
        
        Returns:
            Dictionary containing retrieved documents and metadata
        """
        logger.info(f"ğŸ” Retrieving {n_results} results for query: {query[:50]}...")
        
        results = self.vector_store.query(
            query_text=query,
            n_results=n_results,
            filter_dict=filter_dict
        )
        
        logger.info(f"âœ… Retrieved {len(results['documents'])} documents")
        
        return results
    
    def generate_answer(self, query: str, contexts: List[str], metadatas: List[Dict]) -> str:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹
        
        Args:
            query: Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            contexts: Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ù…Ù† Vector DB
            metadatas: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        
        Returns:
            Generated answer as string
        """
        logger.info("ğŸ¤– Generating answer using LLM...")
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
        context_text = ""
        for i, (ctx, meta) in enumerate(zip(contexts, metadatas), 1):
            source = meta.get('source_doc', 'Unknown')
            article = meta.get('article_number', '')
            
            context_text += f"\n\n--- Ù…Ø³ØªÙ†Ø¯ {i} "
            if article:
                context_text += f"({article}) "
            context_text += f"(Ø§Ù„Ù…ØµØ¯Ø±: {source}) ---\n{ctx}"
        
        # System prompt Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ
        system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø§Øª Ø§Ù„Ù…ØµØ±ÙŠ.

Ù…Ù‡Ø§Ù…Ùƒ:
1. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­ ÙÙ‚Ø·
2. Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ø¨ÙˆØ¶ÙˆØ­
3. Ø§Ù„ØªÙˆØ¶ÙŠØ­ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ØºÙŠØ± ÙƒØ§ÙÙ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
4. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙˆØ§Ø¶Ø­Ø©

Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø©:
- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚
- Ø§Ø°ÙƒØ± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø§Ù„Ù…ØµØ§Ø¯Ø± (Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯)
- Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙƒØ§ÙÙŠØ§Ù‹ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­
- ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰"""

        user_prompt = f"""Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ØªØ§Ø­:
{context_text}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ù‚Ù… Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…ØªØ§Ø­ Ø£Ø¹Ù„Ø§Ù‡. 
Ø§Ø°ÙƒØ± Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„ØªÙŠ Ø§Ø³ØªØ®Ø¯Ù…ØªÙ‡Ø§ ÙÙŠ Ø¥Ø¬Ø§Ø¨ØªÙƒ."""

        try:
            # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=self.temperature,
                max_tokens=1000
            )
            
            answer = response.choices[0].message.content
            logger.info("âœ… Answer generated successfully")
            
            return answer
            
        except Exception as e:
            logger.error(f"âŒ Error generating answer: {e}")
            return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {str(e)}"
    
    def query(self, question: str, n_results: int = 5, include_sources: bool = True) -> Dict:
        """
        Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„ÙƒØ§Ù…Ù„: Ø§Ø³ØªØ±Ø¬Ø§Ø¹ + ØªÙˆÙ„ÙŠØ¯
        
        Args:
            question: Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            n_results: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ù† Vector DB
            include_sources: Ù‡Ù„ Ù†Ø¶Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        
        Returns:
            {
                "question": str,
                "answer": str,
                "sources": List[Dict],  # Ø¥Ø°Ø§ include_sources=True
                "contexts_used": int
            }
        """
        logger.info(f"ğŸ“ Processing query: {question[:100]}...")
        
        # Step 1: Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        retrieval_results = self.retrieve(question, n_results)
        
        # Step 2: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        answer = self.generate_answer(
            query=question,
            contexts=retrieval_results['documents'],
            metadatas=retrieval_results['metadatas']
        )
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø©
        result = {
            "question": question,
            "answer": answer,
            "contexts_used": len(retrieval_results['documents'])
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¥Ø°Ø§ Ù…Ø·Ù„ÙˆØ¨
        if include_sources:
            sources = []
            for doc, meta, dist in zip(
                retrieval_results['documents'],
                retrieval_results['metadatas'],
                retrieval_results['distances']
            ):
                source_info = {
                    "source_doc": meta.get('source_doc', 'Unknown'),
                    "chunk_type": meta.get('chunk_type', 'Unknown'),
                    "relevance_score": float(1 - dist),  # ØªØ­ÙˆÙŠÙ„ distance Ø¥Ù„Ù‰ similarity
                    "preview": doc[:200] + "..." if len(doc) > 200 else doc
                }
                
                # Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ø§Ù„Ù…Ø§Ø¯Ø© Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯
                if 'article_number' in meta:
                    source_info['article_number'] = meta['article_number']
                
                sources.append(source_info)
            
            result['sources'] = sources
        
        logger.info("âœ… Query completed successfully")
        
        return result
    
    def batch_query(self, questions: List[str], n_results: int = 5) -> List[Dict]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ø£Ø³Ø¦Ù„Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
        
        Args:
            questions: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            n_results: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„
        
        Returns:
            List of query results
        """
        logger.info(f"ğŸ“š Processing batch of {len(questions)} questions...")
        
        results = []
        for i, question in enumerate(questions, 1):
            logger.info(f"Processing question {i}/{len(questions)}")
            try:
                result = self.query(question, n_results=n_results)
                results.append(result)
            except Exception as e:
                logger.error(f"Error processing question {i}: {e}")
                results.append({
                    "question": question,
                    "answer": f"Error: {str(e)}",
                    "contexts_used": 0
                })
        
        logger.info(f"âœ… Batch processing complete: {len(results)} results")
        
        return results


# Example usage and testing
if __name__ == "__main__":
    import json
    
    print("\n" + "="*60)
    print("Testing Legal RAG Retriever")
    print("="*60 + "\n")
    
    try:
        # Initialize vector store
        print("ğŸ”„ Loading vector store...")
        vector_store = LegalVectorStore(db_path="data/vector_db/chroma_db")
        
        # Initialize RAG retriever
        print("ğŸ”„ Initializing RAG retriever...")
        rag = LegalRAGRetriever(vector_store)
        
        # Test queries
        test_questions = [
            "Ù…Ø§ Ù‡ÙŠ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø³Ø±Ù‚Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠØŸ",
            "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù‚ØªÙ„ Ø§Ù„Ø¹Ù…Ø¯ ÙˆØ§Ù„Ù‚ØªÙ„ Ø§Ù„Ø®Ø·Ø£ØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…Ø´Ø¯Ø¯Ø© ÙÙŠ Ø¬Ø±ÙŠÙ…Ø© Ø§Ù„Ø³Ø±Ù‚Ø©ØŸ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{'='*60}")
            print(f"Test Query {i}/{len(test_questions)}")
            print(f"{'='*60}\n")
            
            result = rag.query(question, n_results=3)
            
            print(f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {result['question']}\n")
            print(f"âœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n{result['answer']}\n")
            print(f"ğŸ“š Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {result['contexts_used']}\n")
            
            if 'sources' in result:
                print("ğŸ“„ Ø§Ù„Ù…ØµØ§Ø¯Ø±:")
                for j, source in enumerate(result['sources'], 1):
                    print(f"  {j}. {source['source_doc']} "
                          f"(Ø¯Ù‚Ø©: {source['relevance_score']:.2%})")
                    if 'article_number' in source:
                        print(f"     {source['article_number']}")
        
        print(f"\n{'='*60}")
        print("âœ… All tests completed successfully!")
        print(f"{'='*60}\n")
        
    except ValueError as e:
        print(f"\nâŒ Error: {e}")
        print("\nTo fix this:")
        print("1. Set your OpenAI API key:")
        print("   export OPENAI_API_KEY='your-key-here'")
        print("2. Or create a .env file with:")
        print("   OPENAI_API_KEY=your-key-here\n")
    
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()